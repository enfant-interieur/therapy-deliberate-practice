{
  "models": [
    {
      "id": "local//llm/qwen3-hf",
      "kind": "llm",
      "display": {
        "title": "Qwen3 HF",
        "description": "Qwen3-4B inference via transformers AutoModel on Unsloth's GGUF build.",
        "tags": [
          "qwen",
          "hf",
          "local"
        ],
        "icon": "bolt"
      },
      "compat": {
        "platforms": [
          "darwin-x64",
          "windows-x64",
          "linux-x64"
        ],
        "acceleration": [
          "cpu",
          "cuda"
        ],
        "priority": 90,
        "requires_ram_gb": 8,
        "requires_vram_gb": 0,
        "disk_gb": 10
      },
      "api": {
        "endpoint": "responses",
        "advertised_model_name": "qwen3-hf",
        "supports_stream": true
      },
      "limits": {
        "timeout_sec": 300,
        "concurrency": 1,
        "max_input_mb": 25,
        "max_output_tokens_default": 1024
      },
      "backend": {
        "provider": "hf",
        "model_ref": "unsloth/Qwen3-4B-Instruct-2507-GGUF",
        "revision": null,
        "device_hint": "auto",
        "extra": {}
      },
      "execution": {
        "mode": "inprocess",
        "warmup_on_start": false
      },
      "launch": {
        "enabled": false,
        "type": "command",
        "explain": "Runs in-process via transformers.",
        "env": {},
        "cmd": [
          "python",
          "-m",
          "local_runtime"
        ],
        "ready": {
          "kind": "http",
          "timeout_sec": 60,
          "http_url": "http://127.0.0.1:{port}/health",
          "log_regex": "READY"
        }
      },
      "ui_params": [],
      "deps": {
        "python_extras": [
          "hf"
        ],
        "pip": [
          "transformers>=4.52",
          "torch>=2.3"
        ],
        "system": [],
        "notes": "Requires transformers AutoModel support for Qwen3 GGUF."
      }
    },
    {
      "id": "local//llm/qwen3-mlx",
      "kind": "llm",
      "display": {
        "title": "Qwen3 MLX",
        "description": "Local Qwen3 inference via MLX on Apple Silicon.",
        "tags": [
          "qwen",
          "mlx",
          "local"
        ],
        "icon": "cpu"
      },
      "compat": {
        "platforms": [
          "darwin-arm64"
        ],
        "acceleration": [
          "metal"
        ],
        "priority": 120,
        "requires_ram_gb": 8,
        "requires_vram_gb": 0,
        "disk_gb": 6
      },
      "api": {
        "endpoint": "responses",
        "advertised_model_name": "qwen3-mlx",
        "supports_stream": true
      },
      "limits": {
        "timeout_sec": 300,
        "concurrency": 1,
        "max_input_mb": 25,
        "max_output_tokens_default": 2048
      },
      "backend": {
        "provider": "mlx",
        "model_ref": "Qwen/Qwen3-4B-MLX-4bit",
        "revision": null,
        "device_hint": "metal",
        "extra": {}
      },
      "execution": {
        "mode": "inprocess",
        "warmup_on_start": false
      },
      "launch": {
        "enabled": false,
        "type": "command",
        "explain": "MLX runs in-process.",
        "env": {},
        "cmd": [
          "python",
          "-m",
          "local_runtime"
        ],
        "ready": {
          "kind": "http",
          "timeout_sec": 60,
          "http_url": "http://127.0.0.1:{port}/health",
          "log_regex": "READY"
        }
      },
      "ui_params": [],
      "deps": {
        "python_extras": [
          "mlx"
        ],
        "pip": [
          "mlx-lm>=0.25.2"
        ],
        "system": [],
        "notes": "Requires Apple Silicon with MLX support."
      }
    },
    {
      "id": "local//stt/faster-whisper",
      "kind": "stt",
      "display": {
        "title": "Faster Whisper",
        "description": "Local Whisper transcription for quick offline audio-to-text.",
        "tags": [
          "stt",
          "whisper",
          "local"
        ],
        "icon": "mic"
      },
      "compat": {
        "platforms": [
          "darwin-arm64",
          "darwin-x64",
          "windows-x64",
          "linux-x64"
        ],
        "acceleration": [
          "cpu",
          "cuda",
          "metal"
        ],
        "priority": 120,
        "requires_ram_gb": 6,
        "requires_vram_gb": 4,
        "disk_gb": 4
      },
      "api": {
        "endpoint": "audio.transcriptions",
        "advertised_model_name": "faster-whisper",
        "supports_stream": true
      },
      "limits": {
        "timeout_sec": 300,
        "concurrency": 2,
        "max_input_mb": 25,
        "max_output_tokens_default": 2048
      },
      "backend": {
        "provider": "faster_whisper",
        "model_ref": "base",
        "revision": null,
        "device_hint": "auto",
        "extra": {}
      },
      "execution": {
        "mode": "subprocess",
        "warmup_on_start": false
      },
      "launch": {
        "enabled": false,
        "type": "command",
        "explain": "Runs via worker subprocess.",
        "env": {},
        "cmd": [
          "python",
          "-m",
          "local_runtime"
        ],
        "ready": {
          "kind": "http",
          "timeout_sec": 60,
          "http_url": "http://127.0.0.1:{port}/health",
          "log_regex": "READY"
        }
      },
      "ui_params": [],
      "deps": {
        "python_extras": [
          "stt"
        ],
        "pip": [
          "faster-whisper"
        ],
        "system": [
          "ffmpeg"
        ],
        "notes": "Requires ffmpeg for decoding audio input."
      }
    },
    {
      "id": "local//stt/parakeet-mlx",
      "kind": "stt",
      "display": {
        "title": "Parakeet MLX",
        "description": "Local MLX speech-to-text via the Parakeet TDT model.",
        "tags": [
          "stt",
          "mlx",
          "local"
        ],
        "icon": "mic"
      },
      "compat": {
        "platforms": [
          "darwin-arm64"
        ],
        "acceleration": [
          "metal"
        ],
        "priority": 130,
        "requires_ram_gb": 8,
        "requires_vram_gb": 0,
        "disk_gb": 6
      },
      "api": {
        "endpoint": "audio.transcriptions",
        "advertised_model_name": "parakeet-mlx",
        "supports_stream": true
      },
      "limits": {
        "timeout_sec": 300,
        "concurrency": 1,
        "max_input_mb": 25,
        "max_output_tokens_default": 2048
      },
      "backend": {
        "provider": "mlx",
        "model_ref": "mlx-community/parakeet-tdt-0.6b-v3",
        "revision": null,
        "device_hint": "metal",
        "extra": {}
      },
      "execution": {
        "mode": "inprocess",
        "warmup_on_start": false
      },
      "launch": {
        "enabled": false,
        "type": "command",
        "explain": "MLX runs in-process.",
        "env": {},
        "cmd": [
          "python",
          "-m",
          "local_runtime"
        ],
        "ready": {
          "kind": "http",
          "timeout_sec": 60,
          "http_url": "http://127.0.0.1:{port}/health",
          "log_regex": "READY"
        }
      },
      "ui_params": [],
      "deps": {
        "python_extras": [
          "mlx",
          "stt"
        ],
        "pip": [
          "parakeet-mlx>=0.2.0"
        ],
        "system": [],
        "notes": "Requires parakeet-mlx for transcription."
      }
    }
  ]
}
