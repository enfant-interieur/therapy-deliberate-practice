# Local LLM Service

Python MLX inference server. Exposes:

- `GET /health`
- `POST /evaluate` with deliberate practice payload

Configure with `LOCAL_LLM_MODEL`.
